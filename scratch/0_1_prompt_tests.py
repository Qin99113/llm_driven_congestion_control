# -*- coding: utf-8 -*-
"""0.1-prompt-tests.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16qtBOOobVtGvpEEuEar82OYdyb0s80pV

# 0 Setups
- Option 1: OpenAI (easier)
- Option 2: VLLM (underconstruction)

## 0.1 OpenAI
"""

import ast
from openai import OpenAI
# openai_apikey = input('Your openai API key here:')
import os
import json
os.environ['OPENAI_API_KEY'] = ""


"""# 1. LLM-for-TCP
- TODOs:
    - Add rules to control the output (zero-shot)
    - Few-shot examples
    - Post processing steps (i.e., parsing, filtering, etc.)
"""

# shiyuan modified
# # One to one topo
# few_shot_data = [
#     {
#         "input": "Current network topology with a sender, R1, R2, and a receiver, where: Link from the sender to R1 has a bandwidth of 1000 Mbps with a delay of 5 ms; Link between R1 and R2 (the bottleneck) has a bandwidth of 10 Mbps and a delay of 10 ms; Link from R2 to the receiver has a bandwidth of 1000 Mbps with a delay of 5 ms. The current TCP NewReno parameters overtime are: CWND values: [104256, 224440]; Queuesizes are: [33, 98]; rtt: [+55ns, +188ns]; throughput: [10.08, 14.94]. Output the optimal congestion control for the next time step.",
#         # "input": "Current network topology with a sender, R1, R2, and a receiver, where: \n Sender to R1: Bandwidth 1000 Mbps, Delay 5ms \n R1 to R2(bottleneck link): Bandwidth 100 Mbps, Delay 10ms \n R2 to receiver: Bandwidth 1000 Mbps, Delay 5ms. The current TCP NewReno parameters overtime are: CWND values: [104256, 224440]; Queuesizes are: [33, 98]; rtt: [+55ns, +188ns]; throughput: [10.08, 14.94]. Output the optimal congestion control for the next time step.",
#         "output": "['159280', '46', '+150ns', '9.54']"
#     },
#     # {
#     #     "input": "Current network topology with a sender, R1, R2, and a receiver, where: Link from the sender to R1 has a bandwidth of 1000 Mbps with a delay of 5 ms; Link between R1 and R2 (the bottleneck) has a bandwidth of 10 Mbps and a delay of 10 ms; Link from R2 to the receiver has a bandwidth of 1000 Mbps with a delay of 5 ms. The current TCP NewReno parameters overtime are: CWND values: [156847, 162176, 181550]; Queuesizes are: [72, 75, 90]; rtt: [+129ns, +134ns, +149ns]; throughput: [10.02, 9.96, 10.02]. Output the optimal congestion control for the next time step.",
#     #     "output": "['190687', '95', '+158ns', '9.96']"
#     # },
#     # {
#     #     "input": "Current network topology with a sender, R1, R2, and a receiver, where: Link from the sender to R1 has a bandwidth of 1000 Mbps with a delay of 5 ms; Link between R1 and R2 (the bottleneck) has a bandwidth of 10 Mbps and a delay of 10 ms; Link from R2 to the receiver has a bandwidth of 1000 Mbps with a delay of 5 ms. The current TCP NewReno parameters overtime are: CWND values: [190498, 194842]; Queuesizes are: [95, 98]; rtt: [+158ns, +161ns]; throughput: [9.96, 10.02]. Output the optimal congestion control for the next time step.",
#     #     "output": "['98464', '33', '+81ns', '10.08']"
#     # }
# ]

# # Many to many topo 1
# few_shot_data = [
#     {
#         "input": "Current network topology with three senders S1 S2 S3, two routers R1 R2, and a receiver, where: Links from all senders to R1 have a bandwidth of 1000 Mbps with a delay of 5 ms respectivaly; Link between R1 and R2 (the bottleneck) has a bandwidth of 15 Mbps and a delay of 10 ms; Link from R2 to the receiver has a bandwidth of 1000 Mbps with a delay of 5 ms. The current TCP NewReno parameters overtime are: CWND values: [104256, 224440]; Queuesizes are: [33, 98]; rtt: [+55ns, +188ns]; throughput: [10.08, 14.94]. Output the optimal congestion control for the next time step.",
#         "output": "['159280', '46', '+150ns', '9.54']"
#     },
# ]

# # Many to many topo 2
# few_shot_data = [
#     {
#         "input": "Current network topology with three senders S0 S1 S2, two routers R1 R2, and a receiver, where: Links from sender S0 and S1 to R1 have a bandwidth of 1000 Mbps with a delay of 5 ms respectivaly; Link between R1 and R2 (the bottleneck) has a bandwidth of 15 Mbps and a delay of 10 ms; Link from Sender S2 to R2 has a bandwidth of 1000 Mbps with a delay of 5 ms; Link from R2 to the receiver has a bandwidth of 1000 Mbps with a delay of 5 ms. The current TCP NewReno parameters overtime are: CWND values: [104256, 224440]; Queuesizes are: [33, 98]; rtt: [+55ns, +188ns]; throughput: [10.08, 14.94]. Output the optimal congestion control for the next time step.",
#         "output": "['159280', '46', '+150ns', '9.54']"
#     },
# ]

# Many to many topo 3 # Fairness
few_shot_data = [
    {
        "input": "Current network topology with three senders S0 S1 S2, two routers R1 R2, and three receivers RE0 RE1 RE2, where: Links from all senders to R1 have a bandwidth of 1000 Mbps with a delay of 5 ms respectivaly; Link between R1 and R2 (the bottleneck) has a bandwidth of 15 Mbps and a delay of 10 ms; Link from R2 to all three receivers have a bandwidth of 1000 Mbps with a delay of 5 ms respectively. The current TCP NewReno parameters overtime are: CWND values: [104256, 224440]; Queuesizes are: [33, 98]; rtt: [+55ns, +188ns]; throughput: [10.08, 14.94]. Output the optimal congestion control for the next time step.",
        "output": "['159280', '46', '+150ns', '9.54']"
    },
]

naive_prompt = '''
Imaging you are an expert in network congestion control. You are given the following situation:
{system_setting}

Based on the provided network conditions, TCP NewReno parameters, and performance metrics, suggest optimal congestion control parameters as a Python list in the following order just values without any explanation:
[CWND, trigger_latency_llm_threshold, SSThreshold]
'''.strip()

few_shot_prompt = '''
Imaging you are an expert in network congestion control. You are responsible for coming up with optimal congestion control parameters given a particular situation.

Here are some example of persious expert's work:
{few_shot_examples}

One segment size is 1448 bytes. Your output for CWND should not be less than 1448 bytes.

Now, you are facing the following situation:
{system_setting}

Based on the provided network conditions, TCP NewReno parameters, and performance metrics, suggest optimal congestion control parameters as a Python list in the following order just values without any explanation:
[CWND, trigger_latency_llm_threshold, SSThreshold]
'''.strip()

def make_prompt(template, data):
    return template.format(**data)

def openai_call(prompt, client, config):
    # set parameters
    temperature = config['temperature'] if 'temperature' in config else 0.75
    max_tokens = config['max_tokens'] if 'max_tokens' in config else 256
    frequency_penalty = config['frequency_penalty'] if 'frequency_penalty' in config else 0
    presence_penalty = config['presence_penalty'] if 'presence_penalty' in config else 0
    model = config['model'] if 'model' in config else 'text-dacinvi-003'
    return_logprobs = config['return_logprobs'] if 'return_logprobs' in config else False
    logprobs = True if return_logprobs else None

    messages = [
        {
            "role": "user",
            "content": prompt
        }
    ]
    response = client.chat.completions.create(
        model=model,
        messages = messages,
        temperature=temperature,
        max_tokens=max_tokens,
        frequency_penalty=frequency_penalty,
        presence_penalty=presence_penalty,
        logprobs=logprobs
    )
    completion = response.choices[0].message.content.strip()
    if logprobs:
        logprobs = response.choices[0].logprobs
        return completion, logprobs
    else:
        return completion

class LLM4TCP():
    def __init__(self, config):
        self.config = config
        self.model_name = config.get('model', 'gpt-4o-mini')
        self.client = OpenAI()
        self.prompt_config = config.get('prompt_config', 'naive')
        self.previous_output = None

    def create_few_shot(self):
        
        return '\n\n'.join([
            'Example {i} \n Input: {input}\n Output: {output}'.format(
                i = i,
                input = few_shot_data[i]['input'],
                output = few_shot_data[i]['output'],
            ) ## just put all the few shot data into "Input-Output" format
            for i in range(len(few_shot_data))
        ])

    def prep_prompt(self, input_data):
        if self.prompt_config == 'naive':
            self.prompt_template = naive_prompt
            input_prompts = [
                make_prompt(self.prompt_template, {'system_setting': i['input']})
                for i in input_data
            ]
        elif self.prompt_config == 'few_shot':
            self.prompt_template = few_shot_prompt
            few_shot_examples_str = self.create_few_shot()
            input_prompts = [
                make_prompt(self.prompt_template, {
                    'system_setting': i['input'],
                    'few_shot_examples': few_shot_examples_str
                })
                for i in input_data
            ]
        else:
            raise NotImplementedError

        return input_prompts

    def inference_batch(self, input_prompts):
        llm_output = [openai_call(p, self.client, self.config) for p in input_prompts]
        return llm_output

    # shiyuan
    def post_processing(self, llm_output):
        cleaned_output = []
        for output in llm_output:
          # Assume the output is a list in string format - TODO check if the output is always well-formated 
          output_data = None
          try:
              # Safely parse the LLM output string
              output_data = ast.literal_eval(output)
          except (ValueError, SyntaxError):
              print('[Error] ast.literal_eval() failed! output:', output)
              pass
          if output_data is not None:
              # print('[INFO] output_data is not None! output_data:', output_data)
              cleaned_output.append({
                  'cwnd': output_data[0] if len(output_data) > 0 else None,
                  'trigger_latency_llm_threshold': output_data[1] if len(output_data) > 1 else None,
                  'ssthreshold': output_data[2] if len(output_data) > 2 else None,
              })

        return cleaned_output
    
    # shiyuan
    def read_history_and_write_output(self):
        # Read history.txt to get the input parameters
        with open('./scratch/history.txt', 'r') as file:
            history_lines = file.readlines()

        history_data = {
            'topology_description': history_lines[0].strip(),
            'base_algorithm': history_lines[1].strip(),
            'cwnd': history_lines[2].strip(),
            'ssthreshold': history_lines[3].strip(),
            'RTT': history_lines[4].strip(),
            'throughput': history_lines[5].strip(),
            'current_trigger_latency_llm_threshold': history_lines[6].strip(),
        }

        # Run prediction
        input_data = [{'input': history_data}]
        cleaned_output = self.predict_batch(input_data)
        print(cleaned_output)
        # Write llm_output.txt with the required fields in simple text format
        with open('./scratch/llm_output.txt', 'w') as output_file:
            for output in cleaned_output:
                if isinstance(output, dict):
                    output_file.write(f"CWND: {output.get('cwnd', 'N/A')}\n")
                    output_file.write(f"Trigger_latency_llm_threshold: {output.get('trigger_latency_llm_threshold', 'N/A')}\n")
                    output_file.write(f"SSThreshold: {output.get('ssthreshold', 'N/A')}\n\n")

    def predict_batch(self, input_data):
        prompts = self.prep_prompt(input_data)
        llm_output = self.inference_batch(prompts)
        print("llm_output: ", llm_output)
        cleaned_output = self.post_processing(llm_output)
        return cleaned_output
    

######################################################
#################### ATTENTION!!! ####################
############## Verify Before Running !!! #############
############# Check config dictionary !!! ############
######################################################

# naive_config = {
#     'model': 'gpt-4o-mini'
# }

few_shot_config = {
    'prompt_config': 'few_shot',
    'model': 'gpt-4o-mini'
}



############# Running LLM4TCP ############
naive_llm = LLM4TCP(few_shot_config)
# shiyuan
naive_llm.read_history_and_write_output()

