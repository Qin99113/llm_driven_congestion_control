# -*- coding: utf-8 -*-
"""0.1-prompt-tests.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16qtBOOobVtGvpEEuEar82OYdyb0s80pV

# 0 Setups
- Option 1: OpenAI (easier)
- Option 2: VLLM (underconstruction)

## 0.1 OpenAI
"""

from openai import OpenAI
# openai_apikey = input('Your openai API key here:')
import os
import json
os.environ['OPENAI_API_KEY'] = ""


"""# 1. LLM-for-TCP
- TODOs:
    - Add rules to control the output (zero-shot)
    - Few-shot examples
    - Post processing steps (i.e., parsing, filtering, etc.)
"""

# shiyuan modified
test_data = [
    {
        "input": "Imagine you are an expert in network congestion control. You are given the following situation: Current network topology with a sender, R1, R2, and a receiver, where: Link from the sender to R1 has a bandwidth of 1000 Mbps with a delay of 5 ms; Link between R1 and R2 (the bottleneck) has a bandwidth of 10 Mbps and a delay of 10 ms; Link from R2 to the receiver has a bandwidth of 1000 Mbps with a delay of 5 ms. Current TCP NewReno Parameters: Congestion Window (CWND) development over time: [[10, 12, 14, 18, 20, 22, 24]]; Loss Timeout (duration until a retransmission is triggered): [0.2s, 0.3s, 0.25s, 0.4s]; Current Slow Start Threshold (SSThreshold): [20, 22, 18, 15]. The network is experiencing the following conditions based on recent data: Current network latency: [10 ms, 12 ms, 11 ms, 14 ms]; Average throughput over time: [9.8 Mbps, 10.02 Mbps, 9.96 Mbps, 10.14 Mbps]. Based on the provided network conditions, TCP NewReno parameters, and performance metrics, suggest optimal congestion control parameters as a Python list in the following order just values without any explanation:[CWND, trigger_latency_llm_threshold, SSThreshold]",
        "output": ['18', '0.3', '17']
    },
    # {
    #     "input": "The network load is moderate, with occasional spikes. Maintain a balanced approach to minimize packet loss and optimize resource utilization.",
    #     "output": {
    #         "configuration": {
    #             "congestion_control_algorithm": "Cubic",
    #             "max_bandwidth_limit": "1 Gbps",
    #             "latency_optimization": False,
    #             "buffer_size": "medium",
    #             "packet_prioritization": "medium",
    #             "retransmission_timeout": "standard"
    #         }
    #     }
    # },
    # {
    #     "input": "The network is facing a bottleneck at peak times. Minimize packet loss and allocate bandwidth efficiently to handle the congestion.",
    #     "output": {
    #         "configuration": {
    #             "congestion_control_algorithm": "Reno",
    #             "max_bandwidth_limit": "1.5 Gbps",
    #             "latency_optimization": True,
    #             "buffer_size": "high",
    #             "packet_prioritization": "low",
    #             "retransmission_timeout": "medium"
    #         }
    #     }
    # }
]

naive_prompt = '''
Imaging you are an expert in network congestion control. You are given the following situation:
{system_setting}

Based on the provided network conditions, TCP NewReno parameters, and performance metrics, suggest optimal congestion control parameters as a Python list in the following order just values without any explanation:
[CWND, trigger_latency_llm_threshold, SSThreshold]
'''.strip()

def make_prompt(template, data):
    return template.format(**data)

def openai_call(prompt, client, config):
    # set parameters
    temperature = config['temperature'] if 'temperature' in config else 0.75
    max_tokens = config['max_tokens'] if 'max_tokens' in config else 256
    # stop_tokens = config['stop_tokens'] if 'stop_tokens' in config else ['###']
    frequency_penalty = config['frequency_penalty'] if 'frequency_penalty' in config else 0
    presence_penalty = config['presence_penalty'] if 'presence_penalty' in config else 0
    wait_time = config['wait_time']  if 'wait_time' in config else 0
    model = config['model'] if 'model' in config else 'text-dacinvi-003'
    return_logprobs = config['return_logprobs'] if 'return_logprobs' in config else False
    logprobs = True if return_logprobs else None

    messages = [
        {
            "role": "user",
            "content": prompt
        }
    ]
    response = client.chat.completions.create(
        model=model,
        messages = messages,
        temperature=temperature,
        max_tokens=max_tokens,
        # top_p=1,
        # stop_tokens=stop_tokens,
        frequency_penalty=frequency_penalty,
        presence_penalty=presence_penalty,
        logprobs=logprobs
    )
    completion = response.choices[0].message.content.strip()
    if logprobs:
        logprobs = response.choices[0].logprobs
        return completion, logprobs
    else:
        return completion

class LLM4TCP():
    def __init__(self, config):
        self.config = config
        self.model_name = config.get('model', 'gpt-4o-mini')
        if 'gpt' in self.model_name:
            self.client = OpenAI()

        elif 'mistral' in self.model_name:
            self.llm = LLM(
                model="mistralai/Mistral-7B-Instruct-v0.2",
            )
            self.max_length = config.get('max_length', 1024),
            self.temperature = config.get('temperature', 0.001)
        self.prompt_config = config.get('prompt_config', 'naive')
        self.previous_output = None

    def prep_prompt(self, input_data):
        if self.prompt_config == 'naive':
            self.prompt_template = naive_prompt
            input_prompts = [
                make_prompt(self.prompt_template, {'system_setting': i['input']})
                for i in input_data
            ]
        else:
            raise NotImplementedError

        return input_prompts

    def inference_batch(self, input_prompts):
        if 'gpt' in self.model_name:
            llm_output = [openai_call(p, self.client, self.config) for p in input_prompts]
        else:
            sampling_params = SamplingParams(temperature=self.temperature, max_tokens = self.max_length)
            outputs = self.llm.generate(input_prompts, sampling_params)

            # Print the outputs.
            for output in outputs:
                prompt = output.prompt
                generated_text = output.outputs[0].text
                print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
            llm_output = [o.outputs[0].text for o in outputs]
        return llm_output

# shiyuan
    def post_processing(self, llm_output):
        cleaned_output = []
        for output in llm_output:
              # Assume the output is a list in string format
              output_data = json.loads(output)
              cleaned_output.append({
                  'cwnd': output_data[0] if len(output_data) > 0 else None,
                  'trigger_latency_llm_threshold': output_data[1] if len(output_data) > 1 else None,
                  'ssthreshold': output_data[2] if len(output_data) > 2 else None,
              })

        return cleaned_output
# shiyuan
    def read_history_and_write_output(self):
        # Read history.txt to get the input parameters
        with open('./scratch/history.txt', 'r') as file:
            history_lines = file.readlines()

        history_data = {
            'topology_description': history_lines[0].strip(),
            'base_algorithm': history_lines[1].strip(),
            'cwnd': history_lines[2].strip(),
            'ssthreshold': history_lines[3].strip(),
            'RTT': history_lines[4].strip(),
            'throughput': history_lines[5].strip(),
            'current_trigger_latency_llm_threshold': history_lines[6].strip(),
        }

        # Run prediction
        input_data = [{'input': history_data}]
        cleaned_output = self.predict_batch(input_data)
        print(cleaned_output)
        # Write llm_output.txt with the required fields in simple text format
        with open('./scratch/llm_output.txt', 'w') as output_file:
            for output in cleaned_output:
                if isinstance(output, dict):
                    output_file.write(f"CWND: {output.get('cwnd', 'N/A')}\n")
                    output_file.write(f"Trigger_latency_llm_threshold: {output.get('trigger_latency_llm_threshold', 'N/A')}\n")
                    output_file.write(f"SSThreshold: {output.get('ssthreshold', 'N/A')}\n\n")

    # def predict_batch(self, input_data):
    #     if self.previous_output == None:
    #       prompts = self.prep_prompt(input_data)
    #       llm_output = self.inference_batch(prompts)
    #       self.previous_output = llm_output
    #     else:
    #       llm_output = self.previous_output
    #     print(llm_output)
    #     cleaned_output = self.post_processing(llm_output)
    #     return cleaned_output

    def predict_batch(self, input_data):
      prompts = self.prep_prompt(input_data)
      llm_output = self.inference_batch(prompts)
      print(llm_output)
      cleaned_output = self.post_processing(llm_output)
      return cleaned_output

naive_config = {
    'model': 'gpt-4o-mini'
    # 'model': 'mistral'
}
naive_llm = LLM4TCP(naive_config)
# shiyuan
naive_llm.read_history_and_write_output()

